# Data Pipelines with Airflow
  Created high grade data pipelines for a music streaming company , that are dynamic and built from reusable tasks, that can be monitored, and allowed scheduled
  backfilling.The source data resides in S3 and needs to be processed in Sparkify's data warehouse in Amazon Redshift. The source datasets consist of JSON logs 
  that tell about user activity in the application and JSON metadata about the songs the users listen to.

    1) Using Airflow to automate ETL pipelines using Airflow, Python, Amazon Redshift.
    2) Transforming data from various sources into a star schema optimized for the analytics team's use cases.
    3) Writing custom operators to perform tasks such as staging data, filling the data warehouse, and validation through data quality checks.
    4) Setting up IAM Roles, Redshift Clusters, Airflow Connections.
    
    The primary file in this repo is the etl.py, which generates the DAG with all necessary tasks to read in files from S3 buckets, 
    load into staging tables and transform into a star schema which is stored in Redshift.
    
# Operators
      Staging Operator: Using Airflow's PostgreSQL & S3 hooks, data is read and copied to staging tables in redshift.
      It loads JSON formatted files from S3 to Amazon Redshift. The operator creates and runs a SQL COPY statement based on the parameters provided. 
  

      Fact & Dimension Operators: Using Airflow's PostgreSQL hook and variable SQL statements, staging data is transformed into a star schema database
      and stored in appropriate tables in redshift.Dimension loads done with the truncate-insert pattern where the target table is emptied before the load. 
      Fact tables allows append type functionality as they are massive. 

      Data Quality Operator: Using Airflow's PostgreSQL hook to access the newly transformed data, custom SQL commands are run against the tables to
      detect discrepancies within the newly formed data warehouse.
      
# Data


# Song Dataset
      The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song 
      and the artist of that song. The files are partitioned by the first three letters of each song's track ID. 
      For example, here are filepaths to two files in this dataset.

        song_data/A/B/C/TRABCEI128F424C983.json
        song_data/A/A/B/TRAABJL12903CDCF1A.json
        And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

        {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
          
          
 # Log Dataset
          
          
      The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. 
      These simulate app activity logs from a music streaming app based on specified configurations.

       The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

        log_data/2018/11/2018-11-12-events.json
        log_data/2018/11/2018-11-13-events.json
        
  # Schema
  
  # Fact Table
      songplays - records in log data associated with song plays i.e. records with page NextSong
            songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
  # Dimension Tables
      users - users in the app
              user_id, first_name, last_name, gender, level
      songs - songs in music database
              song_id, title, artist_id, year, duration
      artists - artists in music database
              artist_id, name, location, lattitude, longitude
      time - timestamps of records in songplays broken down into specific units
              start_time, hour, day, week, month, year, weekday
 
 
